description:
  repo: TD
  project_name: general_td
  experiment_name: smal_train_big_eval

mlflow: null # если не требуется, то просто убираем этот блок
#  # exp_name - в таком видео в mlflow будет отображаться эксперимент
#  exp_name: ${description.repo}/${description.project_name}/${description.experiment_name}
#  url: https://ml.dbrain.io/ # hostname на котором поднят mlflow сервис
#  username: ds # логин
#  password: EitahsaeghoXooz1Hiev # пароль
#  run_id: null #6ab6434ddf2d4db494490053535611ea # ID запуска в эксперименте, нужен если хотите сохранить веса во время
  # конвертации в конкретный эксперимент, если null будет выбран последний по дате создания
tensorboard:
  log_dir: logs/${description.repo}/${description.project_name}/${description.experiment_name}

data:
  datasets:
    pervichka:
      images_folder: /home/addudkin/clearm_dataset_processing/data/full_text_text_detection/images
      annotation_folder: /home/addudkin/clearm_dataset_processing/data/full_text_text_detection
      p: 1.0
  sample_name:
    train: train_annotation.json
    val: val_annotation.json
  mean: [0.83908421, 0.88127789, 0.90946536]
  std: [0.12534873, 0.12586502, 0.1288737 ]


data_preprocess:
  data_folder: /home/addudkin/clearm_dataset_processing/data/full_text_text_detection/prepared_mask
  augmentation:
    OneOf:
      p: 0.5
      args:
        - DirtyDrumTransform:
            p: 1.0
        - LightingGradientTransform:
            p: 1.0


train:
  epoch: 1000
  image_size: [640, 560]
  batch_size: 16
  workers: 12
  pin_memory: True
  gpu_index: 0


val:
  batch_size: 8
  image_size: [1024, 896]


checkpoint:
  # Количество отслеживаемых лучших checkpoints для дальнейшего усреднения
  average_top_k: 5

# Имя класса используемого датасета из datasets/__init__.py
dataset_name: TextDetPervichka

model:
  class_model: DB
  backbone:
    type: resnet34
    args:
      in_channels: 3
      pretrained: true
  neck:
    type: FPN
    args:
      inner_channels: 256
  head:
    type: DBHead
    args:
      out_channels: 2
      k: 50
  weights: ""

training_type: &training_type multilabel

loss_function:
  type: DBLoss
  params:
    alpha: 5
    beta: 10
    ohem_ratio: 3

optimizer:
  type: AdamW
  params:
    lr: 1.0e-3
    betas: [ 0.9, 0.999 ]
    weight_decay: 0.0001

scheduler:
  # Имя класса
  type: CosineAnnealingLR
  # Передаваемые параметры
  params:
    T_max: ${train.epoch}
    eta_min: 0

border_creator:
  params:
    shrink_ratio: 0.4
    thresh_min: 0.3
    thresh_max: 0.7

mask_shrinker:
  params:
    shrink_ratio: 0.4
    min_text_size: 8
    shrink_type: 'pyclipper'

post_processor:
  params:
    unclip_ratio: 2.25
    binarization_threshold: 0.3
    confidence_threshold: 0.7
    min_area: 10.

train_transforms:
  - ColorJitter:
      p: 0.3
      brightness: 0.12
      saturation: 0.5
  - HorizontalFlip:
      p: 0.3
  - OneOf:
      p: 1.0
      args:
        - ElasticTransform:
            p: 0.3
            border_mode: 0
        - ShiftScaleRotate:
            shift_limit: 0.1
            rotate_limit: 3
            shift_limit_y: 0.1
            shift_limit_x: 0.1
            border_mode: 0
            p: 0.3
  - RandomCrop:
      p: 1
      width: 512
      height: 512

train_pre_transforms: null

val_pre_transforms: null

val_transforms: null

test_transforms: null
